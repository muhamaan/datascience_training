{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを加工するノートブック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 秋野編集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフォルダの場所を設定\n",
    "dataPath = \"/data\"\n",
    "dataPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み出し\n",
    "df = pd.read_csv(dataPath + \"/train.csv\")\n",
    "scores = df[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltkデータ読み込み\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各テキストのベクトルを計算する関数\n",
    "def text_to_vector(text, model):\n",
    "    vectors = [model.wv[word] for word in text if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(train_df):\n",
    "    # テキストとスコアを取得\n",
    "    texts = train_df[\"full_text\"]\n",
    "    \n",
    "\n",
    "    # テキストをトークン化\n",
    "    tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "    # Word2Vecモデルの訓練\n",
    "    word2vec_model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # 各テキストのベクトルを計算\n",
    "    text_vectors = np.array([text_to_vector(text, word2vec_model) for text in tokenized_texts])\n",
    "\n",
    "    return text_vectors, word2vec_model\n",
    "\n",
    "# DataFrameに含まれたテキストデータから、トークン化されたtext_vectorを取得\n",
    "text_vectors, word2vec_model = get_text_vectors(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectors_df = pd.DataFrame(text_vectors)\n",
    "text_vectors_df['score'] = df[['score']].copy()\n",
    "# text_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors_with_model(train_df, word2vec_model):\n",
    "    # テキストとスコアを取得\n",
    "    texts = train_df[\"full_text\"]\n",
    "    \n",
    "    # テキストをトークン化\n",
    "    tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "    # 各テキストのベクトルを計算\n",
    "    text_vectors = np.array([text_to_vector(text, word2vec_model) for text in tokenized_texts])\n",
    "\n",
    "    return text_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_by_randomForest(text_vectors_df, n_estimators, random_state=42):\n",
    "    feature_columns = [i for i in text_vectors_df.columns if i != \"score\"]\n",
    "    train_df = text_vectors_df[feature_columns]\n",
    "    target = text_vectors_df[[\"score\"]]\n",
    "\n",
    "    # データを訓練セットをテストセットに分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_df, target, test_size=0.2, random_state=42)   \n",
    "\n",
    "    # ランダムフォレスト分類器を訓練\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # テストデータに対する予測\n",
    "    y_pred = model.predict(X_test)\n",
    "    kappa_quadratic = cohen_kappa_score(y_test, y_pred, weights=\"quadratic\")\n",
    "    print(\"Weighted Kappa 二乗重み付け：\", kappa_quadratic)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_by_randomForest(text_vectors_df, n_estimators=100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測とSubmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの読み出し\n",
    "test_df = pd.read_csv(dataPath + \"/test.csv\")\n",
    "\n",
    "# DataFrameに含まれたテキストデータから、トークン化されたtext_vectorを取得\n",
    "test_text_vectors = get_text_vectors_with_model(test_df, word2vec_model)\n",
    "\n",
    "# text_vectorを使って、予測の実行\n",
    "test_pred = model.predict(test_text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_df[[\"essay_id\"]].copy()\n",
    "submission_df['score'] = test_pred\n",
    "submission_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 岡本編集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なライブラリインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(df):\n",
    "    \"\"\"特徴量作成関数\n",
    "\n",
    "    特徴量の説明\n",
    "        text_len:テキストの長さ\n",
    "        space_count:空白の数\n",
    "        word_len_avg:一節の平均的な長さ\n",
    "        I-cnt:”私”という単語の出現頻度\n",
    "\n",
    "    Args:\n",
    "        df(pandas.DataFrame):加工したいデータフレーム\n",
    "    Return:\n",
    "        pandas.DataFrame:加工後のデータフレーム\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df['text_len'] = df.full_text.str.len()\n",
    "    df['space_count'] = df.full_text.str.count(' ')\n",
    "    df['word_len_avg'] = (df.text_len - df.space_count) / (df.space_count + 1)\n",
    "    df['I-cnt'] = df.full_text.str.startswith('I') + df.full_text.str.count('. I ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = processing(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = processing(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト翻訳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text):\n",
    "    try:\n",
    "        translated = translator.translate(text,src='en',dest='ja').text\n",
    "        return translated\n",
    "    except Exception as e:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_ja'] = train_df.full_text.progress_apply(translate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#翻訳後データフレームをcsv出力\n",
    "train_df.to_csv('./data/trandlated_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語特徴量作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の出現頻度確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_freq(df):\n",
    "    vec_count = CountVectorizer()\n",
    "    vec_count.fit(df.full_text)\n",
    "    X = vec_count.transform(df.full_text)\n",
    "    word_df = pd.DataFrame(X.toarray(), columns=vec_count.get_feature_names_out())\n",
    "    df = pd.concat([df, word_df], axis=1)\n",
    "    word_df = pd.DataFrame(word_df.sum(axis=0).sort_values(ascending=False).reset_index())\n",
    "    word_df.columns = ['word', 'count']\n",
    "    return df,word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練データのテキストを単語に分解\n",
    "train_df, word_df_train = check_freq(train_df)\n",
    "display(train_df.head())\n",
    "display(word_df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テストデータのテキストを単語に分解\n",
    "test_df, word_df_test = check_freq(test_df)\n",
    "display(test_df.head())\n",
    "display(word_df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ストップワード削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#英語のストップワードダウンロード\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_set = set(stop_words)\n",
    "def drop_stopword(df,columns_set):\n",
    "    \n",
    "    #単語データフレーム内にどれだけストップワードが含まれるのか確認\n",
    "    columns_set = set(df.word)\n",
    "\n",
    "    #共通単語抽出\n",
    "    and_set = columns_set & stop_words_set\n",
    "\n",
    "    #単語データフレームに存在しなかったストップワード数抽出\n",
    "    diff_set = stop_words_set - and_set\n",
    "\n",
    "    #ストップワードの除去、除去前後で矛盾がないか確認\n",
    "    before_count = len(df)\n",
    "    df.drop(columns=list(and_set),inplace=True)\n",
    "    after_count = len(df)\n",
    "    if len(and_set) - (before_count - after_count) == 0:\n",
    "        print('処理に問題はありません')\n",
    "    else:\n",
    "        print('処理後のレコード数が想定しているレコード数と一致しません')\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = drop_stopword(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理後データ出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/processed_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('./data/processed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
